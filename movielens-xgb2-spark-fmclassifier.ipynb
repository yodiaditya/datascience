{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Movielens-1M using FMClassifier and FMRegressor** \n",
    "Compare the prediction using FMClassifier and FMRegressor from Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Loading and Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import FloatType, IntegerType, LongType \n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import FMClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--driver-memory 192g --executor-memory 16g pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/31 16:00:37 WARN Utils: Your hostname, server resolves to a loopback address: 127.0.1.1; using 192.168.18.50 instead (on interface enp36s0f0)\n",
      "24/01/31 16:00:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/31 16:00:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "builder = SparkSession.builder\n",
    "builder = builder.config(\"spark.driver.maxResultSize\", \"5G\")\n",
    "\n",
    "spark = builder.appName(\"FMClassifier_MovieLens\").getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000204</th>\n",
       "      <td>6040</td>\n",
       "      <td>1091</td>\n",
       "      <td>1</td>\n",
       "      <td>956716541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000205</th>\n",
       "      <td>6040</td>\n",
       "      <td>1094</td>\n",
       "      <td>5</td>\n",
       "      <td>956704887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000206</th>\n",
       "      <td>6040</td>\n",
       "      <td>562</td>\n",
       "      <td>5</td>\n",
       "      <td>956704746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000207</th>\n",
       "      <td>6040</td>\n",
       "      <td>1096</td>\n",
       "      <td>4</td>\n",
       "      <td>956715648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000208</th>\n",
       "      <td>6040</td>\n",
       "      <td>1097</td>\n",
       "      <td>4</td>\n",
       "      <td>956715569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000209 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  item_id  rating  timestamp\n",
       "0              1     1193       5  978300760\n",
       "1              1      661       3  978302109\n",
       "2              1      914       3  978301968\n",
       "3              1     3408       4  978300275\n",
       "4              1     2355       5  978824291\n",
       "...          ...      ...     ...        ...\n",
       "1000204     6040     1091       1  956716541\n",
       "1000205     6040     1094       5  956704887\n",
       "1000206     6040      562       5  956704746\n",
       "1000207     6040     1096       4  956715648\n",
       "1000208     6040     1097       4  956715569\n",
       "\n",
       "[1000209 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# schema = StructType(\n",
    "#     (\n",
    "#         StructField(\"user_id\", IntegerType()),\n",
    "#         StructField(\"item_id\", IntegerType()),\n",
    "#         StructField(\"rating\", FloatType()),\n",
    "#         StructField(\"timestamp\", LongType())\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# rating = spark.read.csv(\"ml-1m/ratings.dat\", sep=\"::\", schema=schema)\n",
    "# rating.show(5)\n",
    "\n",
    "# Faster processing using pandas vs spark\n",
    "rating = pd.read_csv(\"ml-1m/ratings.dat\", names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"], delimiter=\"::\", engine=\"python\")\n",
    "rating.columns = ['user_id','item_id','rating','timestamp']\n",
    "rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating.drop(['timestamp'], axis=1, inplace=True)\n",
    "\n",
    "# set all positive interactions to 1\n",
    "df_classification = rating.copy()\n",
    "df_classification['interaction'] = 1\n",
    "df_classification.drop(['rating'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Classification Approach** \n",
    "Since all the user-item considered positive interaction in implicit feedback manner given all interaction is `1`. The assumption here is to generate negative samples to fit with classification method for FMClassifier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "all_users = df_classification.user_id.unique()\n",
    "all_items = df_classification.item_id.unique()\n",
    "\n",
    "negative_instances = []\n",
    "\n",
    "for user in all_users:\n",
    "    user_interacted_item = df_classification[df_classification.user_id == user]['item_id'].unique()\n",
    "    non_interacted_items = set(all_items) - set(user_interacted_item)\n",
    "    for item in non_interacted_items:\n",
    "        negative_instances.append([user, item, 0])\n",
    "\n",
    "num_negatives = len(rating[df_classification['interaction'] == 1])\n",
    "sampled_negatives = random.sample(negative_instances, num_negatives)\n",
    "\n",
    "df_negatives = pd.DataFrame(sampled_negatives, columns=['user_id', 'item_id', 'interaction'])\n",
    "df_negatives\n",
    "\n",
    "columns = ['user_id', 'item_id', 'interaction']\n",
    "balanced_df = pd.concat([df_classification[columns], df_negatives[columns]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+------------+\n",
      "|user_id|item_id|interaction|    features|\n",
      "+-------+-------+-----------+------------+\n",
      "|      1|   1193|          1|[1.0,1193.0]|\n",
      "|      1|    661|          1| [1.0,661.0]|\n",
      "|      1|    914|          1| [1.0,914.0]|\n",
      "|      1|   3408|          1|[1.0,3408.0]|\n",
      "|      1|   2355|          1|[1.0,2355.0]|\n",
      "|      1|   1197|          1|[1.0,1197.0]|\n",
      "|      1|   1287|          1|[1.0,1287.0]|\n",
      "|      1|   2804|          1|[1.0,2804.0]|\n",
      "|      1|    594|          1| [1.0,594.0]|\n",
      "|      1|    919|          1| [1.0,919.0]|\n",
      "|      1|    595|          1| [1.0,595.0]|\n",
      "|      1|    938|          1| [1.0,938.0]|\n",
      "|      1|   2398|          1|[1.0,2398.0]|\n",
      "|      1|   2918|          1|[1.0,2918.0]|\n",
      "|      1|   1035|          1|[1.0,1035.0]|\n",
      "|      1|   2791|          1|[1.0,2791.0]|\n",
      "|      1|   2687|          1|[1.0,2687.0]|\n",
      "|      1|   2018|          1|[1.0,2018.0]|\n",
      "|      1|   3105|          1|[1.0,3105.0]|\n",
      "|      1|   2797|          1|[1.0,2797.0]|\n",
      "+-------+-------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "balanced_df_spark = spark.createDataFrame(balanced_df)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"user_id\", \"item_id\"], outputCol=\"features\")\n",
    "balanced_df_spark = assembler.transform(balanced_df_spark)\n",
    "balanced_df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS FOR RANDOMIZED NEGATIVE AND POSITIVE SAMPLING APPROACHES\n",
    "\n",
    "# import random \n",
    "# import pyspark.sql.functions as F\n",
    "# from pyspark.sql import DataFrame \n",
    "\n",
    "# all_users = df_rating.select(\"user_id\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "# all_items = df_rating.select(\"item_id\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# negative_instances = []\n",
    "\n",
    "# for user in all_users:\n",
    "#     user_item_interaction = df_rating.filter(df_rating.user_id == user).select(\"item_id\").rdd.flatMap(lambda x: x).collect()\n",
    "#     negative_interactions = list(set(all_items) - set(user_item_interaction))\n",
    "\n",
    "#     for item in negative_interactions:\n",
    "#         negative_instances.append((user, item, 0))\n",
    "\n",
    "# num_negatives = df_rating.filter(df_rating.interaction == 1).count()\n",
    "# sampled_negatives = random.sample(negative_instances, num_negatives)\n",
    "\n",
    "# # create a dataframe with all negative instances\n",
    "# df_negatives = spark.createDataFrame(sampled_negatives, schema=schema)\n",
    "\n",
    "# balanced_df = df_rating.select(\"user_id\", \"item_id\", \"interaction\").union(df_negatives)\n",
    "\n",
    "# # merging with all positive data \n",
    "# combined = all_user_item_pairs.join(rating, on=[\"user_id\", \"item_id\"], how=\"left\")\n",
    "# data_with_neg = combined.withColumn(\"interaction\", F.coalesce(combined.interaction, F.lit(0)))\n",
    "\n",
    "# negative_samples = data_with_neg.filter(data_with_neg.interaction == 0)\n",
    "# positive_samples = data_with_neg.filter(data_with_neg.interaction != 0)\n",
    "\n",
    "# fraction = positive_samples.count() / float(negative_samples.count())\n",
    "# balanced_negatives = negative_samples.sample(False, fraction, seed=42)\n",
    "\n",
    "# balanced_data = positive_samples.union(balanced_negatives)\n",
    "\n",
    "# indexer_user = StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_idx\")\n",
    "# indexer_item = StringIndexer(inputCol=\"item_id\", outputCol=\"item_id_idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Columns:  ['user_id', 'item_id', 'interaction', 'features']\n",
      "Test Data Columns:  ['user_id', 'item_id', 'interaction', 'features']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/31 16:01:30 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(train_data, test_data) = balanced_df_spark.randomSplit([0.75, 0.25], seed=42)\n",
    "\n",
    "# Ensure 'user_id' is present\n",
    "print(\"Training Data Columns: \", train_data.columns)\n",
    "print(\"Test Data Columns: \", test_data.columns)\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"interaction\", outputCol=\"indexedLabel\").fit(balanced_df_spark)\n",
    "\n",
    "fm = FMClassifier(featuresCol=\"features\", labelCol=\"indexedLabel\", stepSize=0.001)\n",
    "pipeline = Pipeline(stages=[labelIndexer, fm])\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC: 0.4993986982884071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5167887622606877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Precision: 0.498\n",
      "Weighted Recall: 0.500\n",
      "+----------+------------+-----------+\n",
      "|prediction|indexedLabel|   features|\n",
      "+----------+------------+-----------+\n",
      "|       1.0|         1.0|[1.0,150.0]|\n",
      "|       1.0|         1.0|[1.0,588.0]|\n",
      "|       1.0|         1.0|[1.0,595.0]|\n",
      "|       1.0|         1.0|[1.0,608.0]|\n",
      "|       1.0|         1.0|[1.0,783.0]|\n",
      "+----------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_data)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"interaction\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "print(f\"Area under ROC: {roc_auc}\")\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"interaction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"interaction\", predictionCol=\"prediction\")\n",
    "\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "print(\"Weighted Precision: {:.3f}\".format(precision))\n",
    "print(\"Weighted Recall: {:.3f}\".format(recall))\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[232215  17979]\n",
      " [232225  17656]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = predictions.select(\"prediction\").collect()\n",
    "y_orig = predictions.select(\"interaction\").collect()\n",
    "\n",
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix: \\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "\n",
    "def pivot_table_recommendation(recommendation):\n",
    "    # Pivot the data to get a wide format DataFrame with one row per user and top 10 movie recommendations\n",
    "    pv_rec = recommendation.pivot(index='user_id', columns='rank', values='item_id').reset_index()\n",
    "\n",
    "    # Set user_id as the index\n",
    "    pv_rec.set_index('user_id', inplace=True)\n",
    "\n",
    "    # Remove the 'rank' column\n",
    "    pv_rec.columns.name = None\n",
    "    pv_rec.columns = [f'{int(rank)}' for rank in pv_rec.columns]\n",
    "    pv_rec.index.name = None\n",
    "\n",
    "    return pv_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 249:=======================>                              (21 + 27) / 48]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+----+----+----+----+----+----+----+----+\n",
      "|user_id|   1|   2|   3|   4|   5|   6|   7|   8|   9|  10|\n",
      "+-------+----+----+----+----+----+----+----+----+----+----+\n",
      "|      6|  34| 296| 364| 569| 920|1028|1043|1188|1566|1947|\n",
      "|      7|1196|3107|3753|3089|3378|1381|2384|3473| 632|1202|\n",
      "|      9|  25|  47| 377| 412| 428| 508| 590| 597| 805|1089|\n",
      "|     19|  34|  76| 223| 260| 368| 377| 648| 785|1022|1090|\n",
      "|     22|  47|  81|  95| 104| 163| 180| 256| 312| 333| 368|\n",
      "|     25| 110| 157| 223| 260| 546| 737|1129|1356|1371|1372|\n",
      "|     26|   1|  39|  45| 104| 125| 160| 168| 195| 198| 234|\n",
      "|     27| 318| 541| 858| 905| 910| 926| 930| 955|1198|1225|\n",
      "|     29|  50| 288| 318| 589| 858| 912| 969|1225|1356|1374|\n",
      "|     31| 933| 946| 951|1077|1230|1234|1235|1259|1265|1304|\n",
      "|     32|  50| 247| 457| 589| 608|1343|1683|2571|2916|2959|\n",
      "|     34| 339| 353| 441| 455| 497| 837|1079|1100|1148|1196|\n",
      "|     39| 223| 785|1060|1127|1193|2605|2706|2710|3005|3578|\n",
      "|     43|2369|2581|2605|2961|3298|3302|1909|2985|1442|2302|\n",
      "|     50|2241|3155|3409|3565|3594|3786|1554|3241| 982| 222|\n",
      "|     51|   1|1617|1895|2395|3159|3255|3484|3527|3618|3793|\n",
      "|     52|   6| 112| 165| 292| 349| 353| 368| 380| 423| 457|\n",
      "|     54| 909|1193|1235|1269|2788|2171|2756| 168|1351| 960|\n",
      "|     56|   6|  39| 246| 474| 527| 628| 902|1095|2302|2329|\n",
      "|     57|  70| 441| 661|1196|1199|1221|1228|1544|1894|1976|\n",
      "+-------+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# # Rank the predictions\n",
    "windowSpec = Window.partitionBy(\"user_id\").orderBy(predictions[\"prediction\"].desc())\n",
    "df_test = predictions.withColumn(\"rank\", row_number().over(windowSpec))\n",
    "\n",
    "# Filter to get top 10 predictions for each user\n",
    "top_10_recommendations = df_test.filter(df_test['rank'] <= 10)\n",
    "\n",
    "# Pivot the data to get a wide format DataFrame with one row per user and top 10 movie recommendations\n",
    "pivot_recommendations = top_10_recommendations.groupBy(\"user_id\").pivot(\"rank\").agg({\"item_id\": \"first\"})\n",
    "pivot_recommendations = pivot_recommendations.na.fill(0)\n",
    "pivot_recommendations.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34</td>\n",
       "      <td>296</td>\n",
       "      <td>364</td>\n",
       "      <td>569</td>\n",
       "      <td>920</td>\n",
       "      <td>1028</td>\n",
       "      <td>1043</td>\n",
       "      <td>1188</td>\n",
       "      <td>1566</td>\n",
       "      <td>1947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1196</td>\n",
       "      <td>3107</td>\n",
       "      <td>3753</td>\n",
       "      <td>3089</td>\n",
       "      <td>3378</td>\n",
       "      <td>1381</td>\n",
       "      <td>2384</td>\n",
       "      <td>3473</td>\n",
       "      <td>632</td>\n",
       "      <td>1202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>47</td>\n",
       "      <td>377</td>\n",
       "      <td>412</td>\n",
       "      <td>428</td>\n",
       "      <td>508</td>\n",
       "      <td>590</td>\n",
       "      <td>597</td>\n",
       "      <td>805</td>\n",
       "      <td>1089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>34</td>\n",
       "      <td>76</td>\n",
       "      <td>223</td>\n",
       "      <td>260</td>\n",
       "      <td>368</td>\n",
       "      <td>377</td>\n",
       "      <td>648</td>\n",
       "      <td>785</td>\n",
       "      <td>1022</td>\n",
       "      <td>1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>47</td>\n",
       "      <td>81</td>\n",
       "      <td>95</td>\n",
       "      <td>104</td>\n",
       "      <td>163</td>\n",
       "      <td>180</td>\n",
       "      <td>256</td>\n",
       "      <td>312</td>\n",
       "      <td>333</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6039</th>\n",
       "      <td>50</td>\n",
       "      <td>260</td>\n",
       "      <td>588</td>\n",
       "      <td>783</td>\n",
       "      <td>909</td>\n",
       "      <td>911</td>\n",
       "      <td>922</td>\n",
       "      <td>933</td>\n",
       "      <td>942</td>\n",
       "      <td>955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5703</th>\n",
       "      <td>20</td>\n",
       "      <td>104</td>\n",
       "      <td>155</td>\n",
       "      <td>185</td>\n",
       "      <td>553</td>\n",
       "      <td>922</td>\n",
       "      <td>1179</td>\n",
       "      <td>1200</td>\n",
       "      <td>1209</td>\n",
       "      <td>1227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5744</th>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>48</td>\n",
       "      <td>112</td>\n",
       "      <td>165</td>\n",
       "      <td>181</td>\n",
       "      <td>199</td>\n",
       "      <td>212</td>\n",
       "      <td>196</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5752</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>60</td>\n",
       "      <td>111</td>\n",
       "      <td>169</td>\n",
       "      <td>199</td>\n",
       "      <td>223</td>\n",
       "      <td>228</td>\n",
       "      <td>54</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5775</th>\n",
       "      <td>21</td>\n",
       "      <td>100</td>\n",
       "      <td>208</td>\n",
       "      <td>30</td>\n",
       "      <td>37</td>\n",
       "      <td>246</td>\n",
       "      <td>296</td>\n",
       "      <td>298</td>\n",
       "      <td>344</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6040 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1     2     3     4     5     6     7     8     9    10\n",
       "6       34   296   364   569   920  1028  1043  1188  1566  1947\n",
       "7     1196  3107  3753  3089  3378  1381  2384  3473   632  1202\n",
       "9       25    47   377   412   428   508   590   597   805  1089\n",
       "19      34    76   223   260   368   377   648   785  1022  1090\n",
       "22      47    81    95   104   163   180   256   312   333   368\n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...\n",
       "6039    50   260   588   783   909   911   922   933   942   955\n",
       "5703    20   104   155   185   553   922  1179  1200  1209  1227\n",
       "5744    10    31    48   112   165   181   199   212   196   232\n",
       "5752     1    11    60   111   169   199   223   228    54   139\n",
       "5775    21   100   208    30    37   246   296   298   344   785\n",
       "\n",
       "[6040 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot = pivot_recommendations.toPandas().set_index('user_id')\n",
    "pivot = pivot.rename_axis(None, axis=0)\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION CLASSIFICATION TESTSET ONLY\n",
      "\n",
      "hit_rate: 1.000\n",
      "reciprocal_rank: 1.000\n",
      "dcg: 4.544\n",
      "precision: 1.000\n",
      "recall: 0.146\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "\n",
    "test_users_items = df_test.toPandas().groupby('user_id')['item_id'].apply(set).to_dict()\n",
    "comm_user = pivot.index.values\n",
    "\n",
    "def hit_rate(val_pivot, val_test_user_items, val_comm_user):\n",
    "    hit_rate = np.mean([int(len(set(val_pivot.loc[u]) & val_test_user_items[u]) > 0) for u in val_comm_user])\n",
    "    return hit_rate\n",
    "\n",
    "def reciprocal_rank(val_pivot, val_test_user_items, val_comm_user):\n",
    "    match_indexes = [np.where(pivot.loc[u].isin(set(pivot.loc[u]) & test_users_items[u]))[0] for u in comm_user]\n",
    "    reciprocal_rank = np.mean([1 / (np.min(index) + 1) if len(index) > 0 else 0 for index in match_indexes])\n",
    "\n",
    "    return reciprocal_rank\n",
    "\n",
    "def dcg(val_pivot, val_test_user_items, val_comm_user):\n",
    "    match_indexes = [np.where(val_pivot.loc[u].isin(set(val_pivot.loc[u]) & val_test_user_items[u]))[0] for u in val_comm_user]\n",
    "    discounted_cumulative_gain = np.mean([np.sum(1 / np.log2(index + 2)) if len(index) > 0 else 0 for index in match_indexes])\n",
    "    \n",
    "    return discounted_cumulative_gain\n",
    "\n",
    "def precision(val_pivot, val_test_user_items, val_comm_user):\n",
    "    precision = np.mean([len(set(val_pivot.loc[u]) & val_test_user_items[u]) / len(val_pivot.loc[u]) for u in val_comm_user])\n",
    "\n",
    "    return precision\n",
    "\n",
    "def recall(val_pivot, val_test_user_items, val_comm_user):\n",
    "    recall = np.mean([len(set(val_pivot.loc[u]) & val_test_user_items[u]) / len(test_users_items[u]) for u in val_comm_user])\n",
    "\n",
    "    return recall\n",
    "\n",
    "print(\"EVALUATION CLASSIFICATION TESTSET ONLY\\n\")\n",
    "\n",
    "print(\"hit_rate: {:.3f}\".format(hit_rate(pivot, test_users_items, comm_user)))\n",
    "print(\"reciprocal_rank: {:.3f}\".format(reciprocal_rank(pivot, test_users_items, comm_user)))\n",
    "print(\"dcg: {:.3f}\".format(dcg(pivot, test_users_items, comm_user)))\n",
    "print(\"precision: {:.3f}\".format(precision(pivot, test_users_items, comm_user)))\n",
    "print(\"recall: {:.3f}\".format(recall(pivot, test_users_items, comm_user)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use RankFM Validation Data for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (750042, 2)\n",
      "valid shape: (250167, 2)\n",
      "train users: 6040\n",
      "valid users: 6040\n",
      "cold-start users: set()\n",
      "train items: 3670\n",
      "valid items: 3507\n",
      "cold-start items: {3842, 2308, 2438, 3220, 3607, 2584, 1820, 2845, 2591, 545, 1316, 2214, 1832, 1579, 3376, 1714, 1843, 2226, 2742, 311, 826, 2235, 3517, 1470, 576, 2895, 601, 3291, 989, 1630, 2909, 868, 2277, 2039, 3065, 2556}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "interactions = rating.copy()\n",
    "interactions['random'] = np.random.random(size=len(interactions))\n",
    "test_pct = 0.25\n",
    "\n",
    "train_mask = interactions['random'] <  (1 - test_pct)\n",
    "valid_mask = interactions['random'] >= (1 - test_pct)\n",
    "\n",
    "interactions_train = interactions[train_mask][['user_id', 'item_id']]\n",
    "interactions_valid = interactions[valid_mask][['user_id', 'item_id']]\n",
    "\n",
    "train_users = np.sort(interactions_train.user_id.unique())\n",
    "valid_users = np.sort(interactions_valid.user_id.unique())\n",
    "cold_start_users = set(valid_users) - set(train_users)\n",
    "\n",
    "train_items = np.sort(interactions_train.item_id.unique())\n",
    "valid_items = np.sort(interactions_valid.item_id.unique())\n",
    "cold_start_items = set(valid_items) - set(train_items)\n",
    "\n",
    "print(\"train shape: {}\".format(interactions_train.shape))\n",
    "print(\"valid shape: {}\".format(interactions_valid.shape))\n",
    "\n",
    "print(\"train users: {}\".format(len(train_users)))\n",
    "print(\"valid users: {}\".format(len(valid_users)))\n",
    "print(\"cold-start users: {}\".format(cold_start_users))\n",
    "\n",
    "print(\"train items: {}\".format(len(train_items)))\n",
    "print(\"valid items: {}\".format(len(valid_items)))\n",
    "print(\"cold-start items: {}\".format(cold_start_items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_df_spark = spark.createDataFrame(interactions_valid)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"user_id\", \"item_id\"], outputCol=\"features\")\n",
    "interaction_df_spark = assembler.transform(interaction_df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/31 16:02:33 WARN StringIndexerModel: Input column interaction does not exist during transformation. Skip StringIndexerModel for this column.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+----+----+----+----+----+----+----+----+\n",
      "|user_id|   1|   2|   3|   4|   5|   6|   7|   8|   9|  10|\n",
      "+-------+----+----+----+----+----+----+----+----+----+----+\n",
      "|      1| 661|2804| 938|2398|  48| 588|1907| 783|2692|3114|\n",
      "|      2|1357|1537|2916|1213|2881|3030| 434|3108| 292|1293|\n",
      "|      3|3421|1641|3534|3868|1079| 653|2167|1580|3114|3552|\n",
      "|      4| 480|1196|1198|3418|2366|1387|3527|2947|   0|   0|\n",
      "|      5|1175|1392| 860| 215|1759| 501|3578|3793|1610|2058|\n",
      "|      6|1101|  48|3508|   1|2858| 590| 597|3524|3604|3536|\n",
      "|      7| 648| 861| 589|   6| 442| 733|2353|1196|2571| 457|\n",
      "|      8|  39|2268|3500|3148|1476|2490|1836|   1|2429|1704|\n",
      "|      9|3148|2278|3298|  50|1265| 805|1552| 593| 597| 524|\n",
      "|     10|2622|1320|2124|2054|1252| 720|3868|3501|3363|2496|\n",
      "|     11|1753|1188|2639| 663| 597|1777|1784| 246|2806|3101|\n",
      "|     12| 813| 934|1641|1233| 999|   0|   0|   0|   0|   0|\n",
      "|     13|2987|   2|2135|1196| 736| 165|1356| 329|  10|2686|\n",
      "|     14|3354|2997|2731|2826|2686|2762|2694|   0|   0|   0|\n",
      "|     15|2058|2567|3298|3004| 805| 733|1196| 300| 318|3534|\n",
      "|     16|2485|2568|2724| 266|2761|2394|2975|   0|   0|   0|\n",
      "|     17|1253| 589| 223|3510|2571| 300|3375| 164|2657| 316|\n",
      "|     18|2622|2628|1259|1186|3438|3439|2137|1193|3440|1197|\n",
      "|     19|3421|1753|1756|3930| 720|1327|2568|1339|2716|2571|\n",
      "|     20| 648|3863|1694|2571|1923|1371|1527|  47|   0|   0|\n",
      "+-------+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict the interaction probabilities\n",
    "df_test = model.transform(interaction_df_spark)\n",
    "\n",
    "# Rank the predictions\n",
    "windowSpec = Window.partitionBy(\"user_id\").orderBy(df_test[\"prediction\"].desc())\n",
    "df_test = df_test.withColumn(\"rank\", row_number().over(windowSpec))\n",
    "\n",
    "# Filter to get top 10 predictions for each user\n",
    "top_10_recommendations = df_test.filter(df_test['rank'] <= 10)\n",
    "\n",
    "# Pivot the data to get a wide format DataFrame with one row per user and top 10 movie recommendations\n",
    "pivot_recommendations = top_10_recommendations.groupBy(\"user_id\").pivot(\"rank\").agg({\"item_id\": \"first\"})\n",
    "pivot_recommendations = pivot_recommendations.na.fill(0)\n",
    "pivot_recommendations.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** \n",
    "- RankFM able to generate movie_id based on user_id input, while XGBoost is predict the interaction given user_id and movie_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on RankFM Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>661</td>\n",
       "      <td>2804</td>\n",
       "      <td>938</td>\n",
       "      <td>2398</td>\n",
       "      <td>48</td>\n",
       "      <td>588</td>\n",
       "      <td>1907</td>\n",
       "      <td>783</td>\n",
       "      <td>2692</td>\n",
       "      <td>3114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1357</td>\n",
       "      <td>1537</td>\n",
       "      <td>2916</td>\n",
       "      <td>1213</td>\n",
       "      <td>2881</td>\n",
       "      <td>3030</td>\n",
       "      <td>434</td>\n",
       "      <td>3108</td>\n",
       "      <td>292</td>\n",
       "      <td>1293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3421</td>\n",
       "      <td>1641</td>\n",
       "      <td>3534</td>\n",
       "      <td>3868</td>\n",
       "      <td>1079</td>\n",
       "      <td>653</td>\n",
       "      <td>2167</td>\n",
       "      <td>1580</td>\n",
       "      <td>3114</td>\n",
       "      <td>3552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>480</td>\n",
       "      <td>1196</td>\n",
       "      <td>1198</td>\n",
       "      <td>3418</td>\n",
       "      <td>2366</td>\n",
       "      <td>1387</td>\n",
       "      <td>3527</td>\n",
       "      <td>2947</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1175</td>\n",
       "      <td>1392</td>\n",
       "      <td>860</td>\n",
       "      <td>215</td>\n",
       "      <td>1759</td>\n",
       "      <td>501</td>\n",
       "      <td>3578</td>\n",
       "      <td>3793</td>\n",
       "      <td>1610</td>\n",
       "      <td>2058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>43</td>\n",
       "      <td>597</td>\n",
       "      <td>2406</td>\n",
       "      <td>2474</td>\n",
       "      <td>1124</td>\n",
       "      <td>608</td>\n",
       "      <td>2890</td>\n",
       "      <td>1183</td>\n",
       "      <td>1185</td>\n",
       "      <td>2130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5918</th>\n",
       "      <td>608</td>\n",
       "      <td>3863</td>\n",
       "      <td>1196</td>\n",
       "      <td>1198</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5932</th>\n",
       "      <td>21</td>\n",
       "      <td>1100</td>\n",
       "      <td>1208</td>\n",
       "      <td>3267</td>\n",
       "      <td>493</td>\n",
       "      <td>1379</td>\n",
       "      <td>507</td>\n",
       "      <td>2028</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6030</th>\n",
       "      <td>29</td>\n",
       "      <td>110</td>\n",
       "      <td>2094</td>\n",
       "      <td>920</td>\n",
       "      <td>3070</td>\n",
       "      <td>968</td>\n",
       "      <td>3479</td>\n",
       "      <td>260</td>\n",
       "      <td>3489</td>\n",
       "      <td>2105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6038</th>\n",
       "      <td>3548</td>\n",
       "      <td>1354</td>\n",
       "      <td>2700</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6040 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1     2     3     4     5     6     7     8     9    10\n",
       "1      661  2804   938  2398    48   588  1907   783  2692  3114\n",
       "2     1357  1537  2916  1213  2881  3030   434  3108   292  1293\n",
       "3     3421  1641  3534  3868  1079   653  2167  1580  3114  3552\n",
       "4      480  1196  1198  3418  2366  1387  3527  2947     0     0\n",
       "5     1175  1392   860   215  1759   501  3578  3793  1610  2058\n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...\n",
       "5910    43   597  2406  2474  1124   608  2890  1183  1185  2130\n",
       "5918   608  3863  1196  1198     0     0     0     0     0     0\n",
       "5932    21  1100  1208  3267   493  1379   507  2028     0     0\n",
       "6030    29   110  2094   920  3070   968  3479   260  3489  2105\n",
       "6038  3548  1354  2700     0     0     0     0     0     0     0\n",
       "\n",
       "[6040 rows x 10 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot = pivot_recommendations.toPandas().set_index('user_id')\n",
    "pivot = pivot.rename_axis(None, axis=0)\n",
    "pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** \n",
    "- The results is looks totally differences than RankFM and few is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION CLASSIFICATION ON RANKFM VALIDATION DATASET \n",
      "\n",
      "hit_rate: 1.000\n",
      "reciprocal_rank: 1.000\n",
      "dcg: 4.315\n",
      "precision: 0.928\n",
      "recall: 0.510\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "\n",
    "test_users_items = df_test.toPandas().groupby('user_id')['item_id'].apply(set).to_dict()\n",
    "comm_user = pivot.index.values\n",
    "\n",
    "print(\"EVALUATION CLASSIFICATION ON RANKFM VALIDATION DATASET \\n\")\n",
    "\n",
    "print(\"hit_rate: {:.3f}\".format(hit_rate(pivot, test_users_items, comm_user)))\n",
    "print(\"reciprocal_rank: {:.3f}\".format(reciprocal_rank(pivot, test_users_items, comm_user)))\n",
    "print(\"dcg: {:.3f}\".format(dcg(pivot, test_users_items, comm_user)))\n",
    "print(\"precision: {:.3f}\".format(precision(pivot, test_users_items, comm_user)))\n",
    "print(\"recall: {:.3f}\".format(recall(pivot, test_users_items, comm_user)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** \n",
    "\n",
    "- XGB2 outperform because its leverage rich data from user and item features\n",
    "- Next, we test on unseen data to check whether its over-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction on All Combination Data (excluded training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate all possible user-item pairs\n",
    "unique_users = balanced_df_spark.select(\"user_id\").distinct()\n",
    "unique_items = balanced_df_spark.select(\"item_id\").distinct()\n",
    "user_item_pairs = unique_users.crossJoin(unique_items)\n",
    "\n",
    "# Exclude already rated items (present in the training data)\n",
    "training_user_item_pairs = train_data.select(\"user_id\", \"item_id\")\n",
    "user_item_pairs = user_item_pairs.subtract(training_user_item_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"user_id\", \"item_id\"], outputCol=\"features\")\n",
    "all_user_item_pairs_spark = assembler.transform(user_item_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/31 16:02:41 WARN StringIndexerModel: Input column interaction does not exist during transformation. Skip StringIndexerModel for this column.\n"
     ]
    }
   ],
   "source": [
    "# Predict the interaction probabilities\n",
    "df_test = model.transform(all_user_item_pairs_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank the predictions\n",
    "windowSpec = Window.partitionBy(\"user_id\").orderBy(df_test[\"prediction\"].desc())\n",
    "df_test = df_test.withColumn(\"rank\", row_number().over(windowSpec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 339:====================================================>  (48 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+----+----+----+----+----+----+----+----+\n",
      "|user_id|   1|   2|   3|   4|   5|   6|   7|   8|   9|  10|\n",
      "+-------+----+----+----+----+----+----+----+----+----+----+\n",
      "|     19|  61|2909| 186|2826|3416|1668|3439|1898|2901| 848|\n",
      "|     26|2705| 193| 360|3659|3326|3746| 451|2545|3654| 732|\n",
      "|     29|2917|1344|1960|1017|3876|1922| 172|3124|3717|3293|\n",
      "|     54| 173| 639|1005|3435|3683|3576| 900|3931|2295|1695|\n",
      "|     65| 551| 853| 548| 146| 348| 996| 511| 793| 837| 468|\n",
      "|    112| 445| 418|  39| 153| 373| 268| 209|  20| 169| 131|\n",
      "|    113| 440| 173| 235| 351| 342| 377| 401| 156| 300| 101|\n",
      "|    155| 181| 241| 165| 124| 350| 151| 335| 254| 269| 235|\n",
      "|    167|  89| 324|  56| 304| 190|  25| 213|  68| 177| 163|\n",
      "|    191|   6| 217|  59|  53|  35| 105| 272| 215| 290| 176|\n",
      "|    222| 236|  71| 157| 250|  25|  92| 118|  39| 264| 202|\n",
      "|    243|   4|  10| 139|  11|  34| 147| 232| 111|   7| 100|\n",
      "|    270| 132|  77| 188| 190| 111|  69|  27| 185| 134|  72|\n",
      "|    277|   4| 171| 140| 155| 275| 132|   8| 270|  57| 277|\n",
      "|    278| 262|  85| 173| 136|  49| 106|  94| 201|  39| 255|\n",
      "|    287| 129| 193| 264| 215| 102|  35| 117| 240|   2| 231|\n",
      "|    293|  59| 190|  61| 136| 147|  18| 215|  31| 175| 254|\n",
      "|    296| 210| 111|  44| 187| 150| 156| 118| 206| 173| 149|\n",
      "|    299| 214|  77|  10|  98| 213|  44| 182|  72| 104|  11|\n",
      "|    348|  57| 195| 168| 255| 235|  18| 180| 217| 261| 222|\n",
      "+-------+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter to get top 10 predictions for each user\n",
    "top_10_recommendations = df_test.filter(df_test['rank'] <= 10)\n",
    "\n",
    "# # Pivot the data to get a wide format DataFrame with one row per user and top 10 movie recommendations\n",
    "pivot_recommendations = top_10_recommendations.groupBy(\"user_id\").pivot(\"rank\").agg({\"item_id\": \"first\"})\n",
    "pivot_recommendations = pivot_recommendations.na.fill(0)\n",
    "pivot_recommendations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Unseen Data Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION CLASSIFICATION UNSEEN DATA EXCLUDED TRAINING\n",
      "\n",
      "hit_rate: 0.916\n",
      "reciprocal_rank: 0.454\n",
      "dcg: 1.082\n",
      "precision: 0.232\n",
      "recall: 0.001\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set\n",
    "\n",
    "test_users_items = df_test.toPandas().groupby('user_id')['item_id'].apply(set).to_dict()\n",
    "comm_user = pivot.index.values\n",
    "\n",
    "print(\"EVALUATION CLASSIFICATION UNSEEN DATA EXCLUDED TRAINING\\n\")\n",
    "\n",
    "print(\"hit_rate: {:.3f}\".format(hit_rate(pivot, test_users_items, comm_user)))\n",
    "print(\"reciprocal_rank: {:.3f}\".format(reciprocal_rank(pivot, test_users_items, comm_user)))\n",
    "print(\"dcg: {:.3f}\".format(dcg(pivot, test_users_items, comm_user)))\n",
    "print(\"precision: {:.3f}\".format(precision(pivot, test_users_items, comm_user)))\n",
    "print(\"recall: {:.3f}\".format(recall(pivot, test_users_items, comm_user)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** \n",
    "\n",
    "- Its significant lower when predicting over unseen data and back testing into test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Regression Approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+------------+\n",
      "|user_id|item_id|rating|    features|\n",
      "+-------+-------+------+------------+\n",
      "|      1|   1193|     5|[1.0,1193.0]|\n",
      "|      1|    661|     3| [1.0,661.0]|\n",
      "|      1|    914|     3| [1.0,914.0]|\n",
      "|      1|   3408|     4|[1.0,3408.0]|\n",
      "|      1|   2355|     5|[1.0,2355.0]|\n",
      "+-------+-------+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = rating[['user_id', 'item_id', 'rating']]\n",
    "user_item = df.drop(columns=['rating'])\n",
    "\n",
    "df = spark.createDataFrame(df)\n",
    "va = VectorAssembler(inputCols=['user_id', 'item_id'], outputCol='features')\n",
    "va_df = va.transform(df)\n",
    "va_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import FMRegressor\n",
    "\n",
    "(train_data, test_data) = va_df.randomSplit([0.75, 0.25], seed=42)\n",
    "\n",
    "fm = FMRegressor(featuresCol=\"features\", labelCol=\"rating\", stepSize=0.001)\n",
    "pipeline = Pipeline(stages=[fm])\n",
    "\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+-----------+-------------------+\n",
      "|user_id|item_id|rating|   features|         prediction|\n",
      "+-------+-------+------+-----------+-------------------+\n",
      "|      1|    150|     5|[1.0,150.0]|0.11770156730430315|\n",
      "|      1|    588|     4|[1.0,588.0]|0.45603557509911274|\n",
      "|      1|    595|     5|[1.0,595.0]|0.46144273960724846|\n",
      "|      1|    608|     4|[1.0,608.0]| 0.4714846165509289|\n",
      "|      1|    783|     4|[1.0,783.0]| 0.6066637292543353|\n",
      "+-------+-------+------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_data)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 3.3943799544645676\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"rating\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+----+----+----+----+----+----+----+----+\n",
      "|user_id|   1|   2|   3|   4|   5|   6|   7|   8|   9|  10|\n",
      "+-------+----+----+----+----+----+----+----+----+----+----+\n",
      "|      1|3186|3105|2804|2797|2791|2687|2018|1962|1907|1721|\n",
      "|      2|3418|3256|3107|3068|2728|2353|1968|1957|1945|1690|\n",
      "|      3|2997|2470|1641|1304|1270|1079| 653| 260|   0|   0|\n",
      "|      4|1210| 480|   0|   0|   0|   0|   0|   0|   0|   0|\n",
      "|      5|3793|3728|3578|3513|3266|3083|3081|2952|2858|2762|\n",
      "|      6|3685|3682|3624|3600|3565|3536|3408|3072|2966|2321|\n",
      "|      7|3753|3107|1196|   0|   0|   0|   0|   0|   0|   0|\n",
      "|      8|3265|3148|3107|3105|2858|2688|2600|2396|2320|2291|\n",
      "|      9|3948|3755|3623|3510|3178|2890|2028|1912|1682|1584|\n",
      "|     10|3675|3668|3593|3358|3309|3296|3252|3247|3175|3153|\n",
      "|     11|3448|3418|3396|3148|2883|2795|2746|2355|2321|2302|\n",
      "|     12|2616|1247|1198| 813| 593|   0|   0|   0|   0|   0|\n",
      "|     13|3699|3070|2822|2686|2094|2046|2028|2005|2002|1967|\n",
      "|     14|2997|2920|2731|   0|   0|   0|   0|   0|   0|   0|\n",
      "|     15|3717|3510|3499|3489|3301|3298|3261|3178|3160|3145|\n",
      "|     16|2723|2722|2699|2555|2392|2355|1269| 266|   0|   0|\n",
      "|     17|3927|3827|3744|3704|3699|3662|3578|3576|3527|3503|\n",
      "|     18|3723|3633|3526|3439|3438|3399|3397|3269|3260|2872|\n",
      "|     19|3948|3928|3863|3793|3752|3638|3633|3623|2987|2959|\n",
      "|     20|3578|2641|2028|1371| 110|   0|   0|   0|   0|   0|\n",
      "+-------+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict the interaction probabilities\n",
    "df_test = predictions.select(\"*\")\n",
    "\n",
    "# Rank the predictions\n",
    "windowSpec = Window.partitionBy(\"user_id\").orderBy(df_test[\"prediction\"].desc())\n",
    "df_test = df_test.withColumn(\"rank\", row_number().over(windowSpec))\n",
    "\n",
    "# Filter to get top 10 predictions for each user\n",
    "top_10_recommendations = df_test.filter(df_test['rank'] <= 10)\n",
    "\n",
    "# Pivot the data to get a wide format DataFrame with one row per user and top 10 movie recommendations\n",
    "pivot_recommendations = top_10_recommendations.groupBy(\"user_id\").pivot(\"rank\").agg({\"item_id\": \"first\"})\n",
    "pivot_recommendations = pivot_recommendations.na.fill(0)\n",
    "pivot_recommendations.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3186</td>\n",
       "      <td>3105</td>\n",
       "      <td>2804</td>\n",
       "      <td>2797</td>\n",
       "      <td>2791</td>\n",
       "      <td>2687</td>\n",
       "      <td>2018</td>\n",
       "      <td>1962</td>\n",
       "      <td>1907</td>\n",
       "      <td>1721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3418</td>\n",
       "      <td>3256</td>\n",
       "      <td>3107</td>\n",
       "      <td>3068</td>\n",
       "      <td>2728</td>\n",
       "      <td>2353</td>\n",
       "      <td>1968</td>\n",
       "      <td>1957</td>\n",
       "      <td>1945</td>\n",
       "      <td>1690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2997</td>\n",
       "      <td>2470</td>\n",
       "      <td>1641</td>\n",
       "      <td>1304</td>\n",
       "      <td>1270</td>\n",
       "      <td>1079</td>\n",
       "      <td>653</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1210</td>\n",
       "      <td>480</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3793</td>\n",
       "      <td>3728</td>\n",
       "      <td>3578</td>\n",
       "      <td>3513</td>\n",
       "      <td>3266</td>\n",
       "      <td>3083</td>\n",
       "      <td>3081</td>\n",
       "      <td>2952</td>\n",
       "      <td>2858</td>\n",
       "      <td>2762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5902</th>\n",
       "      <td>3606</td>\n",
       "      <td>2863</td>\n",
       "      <td>2628</td>\n",
       "      <td>2571</td>\n",
       "      <td>2028</td>\n",
       "      <td>1684</td>\n",
       "      <td>1283</td>\n",
       "      <td>1247</td>\n",
       "      <td>1244</td>\n",
       "      <td>1212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>3555</td>\n",
       "      <td>3526</td>\n",
       "      <td>3260</td>\n",
       "      <td>3163</td>\n",
       "      <td>3006</td>\n",
       "      <td>2942</td>\n",
       "      <td>2929</td>\n",
       "      <td>2801</td>\n",
       "      <td>2716</td>\n",
       "      <td>2671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5932</th>\n",
       "      <td>3267</td>\n",
       "      <td>2028</td>\n",
       "      <td>780</td>\n",
       "      <td>507</td>\n",
       "      <td>493</td>\n",
       "      <td>457</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6030</th>\n",
       "      <td>3489</td>\n",
       "      <td>3479</td>\n",
       "      <td>3439</td>\n",
       "      <td>3053</td>\n",
       "      <td>2968</td>\n",
       "      <td>2916</td>\n",
       "      <td>2797</td>\n",
       "      <td>2762</td>\n",
       "      <td>2664</td>\n",
       "      <td>2641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6038</th>\n",
       "      <td>2146</td>\n",
       "      <td>1276</td>\n",
       "      <td>1210</td>\n",
       "      <td>1183</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6038 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1     2     3     4     5     6     7     8     9    10\n",
       "1     3186  3105  2804  2797  2791  2687  2018  1962  1907  1721\n",
       "2     3418  3256  3107  3068  2728  2353  1968  1957  1945  1690\n",
       "3     2997  2470  1641  1304  1270  1079   653   260     0     0\n",
       "4     1210   480     0     0     0     0     0     0     0     0\n",
       "5     3793  3728  3578  3513  3266  3083  3081  2952  2858  2762\n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...\n",
       "5902  3606  2863  2628  2571  2028  1684  1283  1247  1244  1212\n",
       "5910  3555  3526  3260  3163  3006  2942  2929  2801  2716  2671\n",
       "5932  3267  2028   780   507   493   457     0     0     0     0\n",
       "6030  3489  3479  3439  3053  2968  2916  2797  2762  2664  2641\n",
       "6038  2146  1276  1210  1183     0     0     0     0     0     0\n",
       "\n",
       "[6038 rows x 10 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot = pivot_recommendations.toPandas().set_index('user_id')\n",
    "pivot = pivot.rename_axis(None, axis=0)\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION CLASSIFICATION ON TEST VALIDATION DATASET \n",
      "\n",
      "hit_rate: 1.000\n",
      "reciprocal_rank: 1.000\n",
      "dcg: 4.309\n",
      "precision: 0.926\n",
      "recall: 0.511\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "\n",
    "test_users_items = df_test.toPandas().groupby('user_id')['item_id'].apply(set).to_dict()\n",
    "comm_user = pivot.index.values\n",
    "\n",
    "print(\"EVALUATION CLASSIFICATION ON TEST VALIDATION DATASET \\n\")\n",
    "\n",
    "print(\"hit_rate: {:.3f}\".format(hit_rate(pivot, test_users_items, comm_user)))\n",
    "print(\"reciprocal_rank: {:.3f}\".format(reciprocal_rank(pivot, test_users_items, comm_user)))\n",
    "print(\"dcg: {:.3f}\".format(dcg(pivot, test_users_items, comm_user)))\n",
    "print(\"precision: {:.3f}\".format(precision(pivot, test_users_items, comm_user)))\n",
    "print(\"recall: {:.3f}\".format(recall(pivot, test_users_items, comm_user)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on RankFM Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+----+----+----+----+----+----+----+----+\n",
      "|user_id|   1|   2|   3|   4|   5|   6|   7|   8|   9|  10|\n",
      "+-------+----+----+----+----+----+----+----+----+----+----+\n",
      "|      1|3114|2804|2692|2398|1907|1246| 938| 783| 661| 608|\n",
      "|      2|3257|3108|3030|2916|2881|2717|2321|2278|2028|1953|\n",
      "|      3|3868|3671|3552|3534|3421|3114|2871|2167|1968|1641|\n",
      "|      4|3527|3418|2947|2366|1387|1198|1196| 480|   0|   0|\n",
      "|      5|3793|3786|3624|3578|3513|3476|3418|3409|3266|3249|\n",
      "|      6|3604|3536|3524|3508|3408|2966|2858|2396|2100|1947|\n",
      "|      7|2571|2353|1221|1196| 861| 733| 648| 589| 480| 457|\n",
      "|      8|3528|3500|3418|3186|3148|2692|2490|2429|2396|2324|\n",
      "|      9|3916|3298|3178|3160|3148|2890|2692|2302|2278|1961|\n",
      "|     10|3868|3812|3702|3701|3688|3671|3591|3501|3451|3447|\n",
      "|     11|3755|3418|3255|3107|3105|3101|2959|2918|2907|2806|\n",
      "|     12|1641|1233| 999| 934| 813|   0|   0|   0|   0|   0|\n",
      "|     13|3256|2987|2916|2871|2686|2528|2470|2414|2135|2115|\n",
      "|     14|3354|2997|2826|2762|2731|2694|2686|   0|   0|   0|\n",
      "|     15|3773|3646|3623|3534|3418|3298|3105|3004|2959|2676|\n",
      "|     16|2975|2761|2724|2568|2485|2394| 266|   0|   0|   0|\n",
      "|     17|3755|3703|3702|3696|3510|3375|3300|3256|3246|3175|\n",
      "|     18|3703|3698|3633|3623|3440|3439|3438|3418|3260|2959|\n",
      "|     19|3930|3744|3638|3527|3421|3264|3208|2860|2858|2836|\n",
      "|     20|3863|2571|1923|1694|1527|1371| 648|  47|   0|   0|\n",
      "+-------+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict the interaction probabilities\n",
    "df_test = model.transform(interaction_df_spark)\n",
    "\n",
    "# Rank the predictions\n",
    "windowSpec = Window.partitionBy(\"user_id\").orderBy(df_test[\"prediction\"].desc())\n",
    "df_test = df_test.withColumn(\"rank\", row_number().over(windowSpec))\n",
    "\n",
    "# Filter to get top 10 predictions for each user\n",
    "top_10_recommendations = df_test.filter(df_test['rank'] <= 10)\n",
    "\n",
    "# Pivot the data to get a wide format DataFrame with one row per user and top 10 movie recommendations\n",
    "pivot_recommendations = top_10_recommendations.groupBy(\"user_id\").pivot(\"rank\").agg({\"item_id\": \"first\"})\n",
    "pivot_recommendations = pivot_recommendations.na.fill(0)\n",
    "pivot_recommendations.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3114</td>\n",
       "      <td>2804</td>\n",
       "      <td>2692</td>\n",
       "      <td>2398</td>\n",
       "      <td>1907</td>\n",
       "      <td>1246</td>\n",
       "      <td>938</td>\n",
       "      <td>783</td>\n",
       "      <td>661</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3257</td>\n",
       "      <td>3108</td>\n",
       "      <td>3030</td>\n",
       "      <td>2916</td>\n",
       "      <td>2881</td>\n",
       "      <td>2717</td>\n",
       "      <td>2321</td>\n",
       "      <td>2278</td>\n",
       "      <td>2028</td>\n",
       "      <td>1953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3868</td>\n",
       "      <td>3671</td>\n",
       "      <td>3552</td>\n",
       "      <td>3534</td>\n",
       "      <td>3421</td>\n",
       "      <td>3114</td>\n",
       "      <td>2871</td>\n",
       "      <td>2167</td>\n",
       "      <td>1968</td>\n",
       "      <td>1641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3527</td>\n",
       "      <td>3418</td>\n",
       "      <td>2947</td>\n",
       "      <td>2366</td>\n",
       "      <td>1387</td>\n",
       "      <td>1198</td>\n",
       "      <td>1196</td>\n",
       "      <td>480</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3793</td>\n",
       "      <td>3786</td>\n",
       "      <td>3624</td>\n",
       "      <td>3578</td>\n",
       "      <td>3513</td>\n",
       "      <td>3476</td>\n",
       "      <td>3418</td>\n",
       "      <td>3409</td>\n",
       "      <td>3266</td>\n",
       "      <td>3249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5910</th>\n",
       "      <td>3176</td>\n",
       "      <td>3125</td>\n",
       "      <td>3102</td>\n",
       "      <td>2942</td>\n",
       "      <td>2919</td>\n",
       "      <td>2890</td>\n",
       "      <td>2716</td>\n",
       "      <td>2611</td>\n",
       "      <td>2474</td>\n",
       "      <td>2406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5918</th>\n",
       "      <td>3863</td>\n",
       "      <td>1198</td>\n",
       "      <td>1196</td>\n",
       "      <td>608</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5932</th>\n",
       "      <td>3267</td>\n",
       "      <td>2028</td>\n",
       "      <td>1379</td>\n",
       "      <td>1208</td>\n",
       "      <td>1100</td>\n",
       "      <td>507</td>\n",
       "      <td>493</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6030</th>\n",
       "      <td>3489</td>\n",
       "      <td>3479</td>\n",
       "      <td>3070</td>\n",
       "      <td>2762</td>\n",
       "      <td>2615</td>\n",
       "      <td>2571</td>\n",
       "      <td>2311</td>\n",
       "      <td>2253</td>\n",
       "      <td>2161</td>\n",
       "      <td>2105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6038</th>\n",
       "      <td>3548</td>\n",
       "      <td>2700</td>\n",
       "      <td>1354</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6040 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         1     2     3     4     5     6     7     8     9    10\n",
       "1     3114  2804  2692  2398  1907  1246   938   783   661   608\n",
       "2     3257  3108  3030  2916  2881  2717  2321  2278  2028  1953\n",
       "3     3868  3671  3552  3534  3421  3114  2871  2167  1968  1641\n",
       "4     3527  3418  2947  2366  1387  1198  1196   480     0     0\n",
       "5     3793  3786  3624  3578  3513  3476  3418  3409  3266  3249\n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...\n",
       "5910  3176  3125  3102  2942  2919  2890  2716  2611  2474  2406\n",
       "5918  3863  1198  1196   608     0     0     0     0     0     0\n",
       "5932  3267  2028  1379  1208  1100   507   493    21     0     0\n",
       "6030  3489  3479  3070  2762  2615  2571  2311  2253  2161  2105\n",
       "6038  3548  2700  1354     0     0     0     0     0     0     0\n",
       "\n",
       "[6040 rows x 10 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot = pivot_recommendations.toPandas().set_index('user_id')\n",
    "pivot = pivot.rename_axis(None, axis=0)\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION CLASSIFICATION ON RANKFM VALIDATION DATASET \n",
      "\n",
      "hit_rate: 1.000\n",
      "reciprocal_rank: 1.000\n",
      "dcg: 4.315\n",
      "precision: 0.928\n",
      "recall: 0.510\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "\n",
    "test_users_items = df_test.toPandas().groupby('user_id')['item_id'].apply(set).to_dict()\n",
    "comm_user = pivot.index.values\n",
    "\n",
    "print(\"EVALUATION CLASSIFICATION ON RANKFM VALIDATION DATASET \\n\")\n",
    "\n",
    "print(\"hit_rate: {:.3f}\".format(hit_rate(pivot, test_users_items, comm_user)))\n",
    "print(\"reciprocal_rank: {:.3f}\".format(reciprocal_rank(pivot, test_users_items, comm_user)))\n",
    "print(\"dcg: {:.3f}\".format(dcg(pivot, test_users_items, comm_user)))\n",
    "print(\"precision: {:.3f}\".format(precision(pivot, test_users_items, comm_user)))\n",
    "print(\"recall: {:.3f}\".format(recall(pivot, test_users_items, comm_user)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 694:=====================================================> (49 + 1) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+----+----+----+----+----+----+----+----+\n",
      "|user_id|   1|   2|   3|   4|   5|   6|   7|   8|   9|  10|\n",
      "+-------+----+----+----+----+----+----+----+----+----+----+\n",
      "|     19|3952|3951|3950|3949|3948|3947|3946|3945|3944|3943|\n",
      "|     26|3952|3951|3950|3949|3948|3947|3946|3945|3944|3943|\n",
      "|     29|3952|3951|3950|3949|3948|3947|3946|3945|3944|3943|\n",
      "|     54|3952|3951|3950|3949|3948|3947|3946|3945|3944|3943|\n",
      "|     65|3952|3951|3950|3949|3947|3946|3945|3944|3943|3942|\n",
      "|    191|3952|3951|3950|3949|3948|3947|3945|3944|3943|3942|\n",
      "|    222|3952|3951|3950|3949|3946|3945|3944|3943|3942|3941|\n",
      "|    243|3952|3951|3950|3949|3948|3947|3946|3945|3944|3943|\n",
      "|    270|3952|3951|3950|3949|3947|3946|3945|3944|3943|3942|\n",
      "|    278|3952|3951|3950|3949|3948|3947|3946|3945|3943|3942|\n",
      "|    293|3952|3951|3950|3949|3947|3946|3945|3944|3943|3942|\n",
      "|    296|3952|3951|3950|3949|3947|3946|3945|3944|3943|3942|\n",
      "|    367|3952|3951|3950|3949|3948|3947|3946|3945|3944|3943|\n",
      "|    418|3952|3951|3950|3949|3948|3947|3946|3945|3944|3942|\n",
      "|    442|3952|3951|3950|3948|3947|3946|3945|3944|3943|3942|\n",
      "|    474|3952|3951|3950|3949|3947|3946|3945|3944|3943|3942|\n",
      "|    541|3952|3950|3949|3948|3947|3945|3944|3943|3942|3941|\n",
      "|    558|3952|3951|3950|3949|3948|3947|3946|3945|3944|3943|\n",
      "|    705|3952|3951|3950|3949|3948|3947|3946|3945|3944|3943|\n",
      "|    720|3952|3951|3950|3949|3948|3947|3946|3945|3944|3943|\n",
      "+-------+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Predict the interaction probabilities\n",
    "df_test = model.transform(all_user_item_pairs_spark)\n",
    "\n",
    "# Rank the predictions\n",
    "windowSpec = Window.partitionBy(\"user_id\").orderBy(df_test[\"prediction\"].desc())\n",
    "df_test = df_test.withColumn(\"rank\", row_number().over(windowSpec))\n",
    "\n",
    "# Filter to get top 10 predictions for each user\n",
    "top_10_recommendations = df_test.filter(df_test['rank'] <= 10)\n",
    "\n",
    "# # Pivot the data to get a wide format DataFrame with one row per user and top 10 movie recommendations\n",
    "pivot_recommendations = top_10_recommendations.groupBy(\"user_id\").pivot(\"rank\").agg({\"item_id\": \"first\"})\n",
    "pivot_recommendations = pivot_recommendations.na.fill(0)\n",
    "pivot_recommendations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION CLASSIFICATION UNSEEN DATA EXCLUDED TRAINING\n",
      "\n",
      "hit_rate: 0.915\n",
      "reciprocal_rank: 0.445\n",
      "dcg: 1.064\n",
      "precision: 0.230\n",
      "recall: 0.001\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set\n",
    "\n",
    "test_users_items = df_test.toPandas().groupby('user_id')['item_id'].apply(set).to_dict()\n",
    "comm_user = pivot.index.values\n",
    "\n",
    "print(\"EVALUATION CLASSIFICATION UNSEEN DATA EXCLUDED TRAINING\\n\")\n",
    "\n",
    "print(\"hit_rate: {:.3f}\".format(hit_rate(pivot, test_users_items, comm_user)))\n",
    "print(\"reciprocal_rank: {:.3f}\".format(reciprocal_rank(pivot, test_users_items, comm_user)))\n",
    "print(\"dcg: {:.3f}\".format(dcg(pivot, test_users_items, comm_user)))\n",
    "print(\"precision: {:.3f}\".format(precision(pivot, test_users_items, comm_user)))\n",
    "print(\"recall: {:.3f}\".format(recall(pivot, test_users_items, comm_user)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Result Comparison**\n",
    "\n",
    "| Metrics | FMClassifier (test data) | FMClassifier (all unseen data) | FM Regressor (test data) | FM Regressor (all unseen data) | RankFM |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| hit_rate | 1.000 |  0.916 | 1.000 | 0.915 | 0.788 |\n",
    "| reciprocal_rank | 1.000 | 0.454 | 1.000 | 0.445 | 0.334 |\n",
    "| dcg | 4.544 | 1.082 | 4.309 | 1.064 | 0.718 |\n",
    "| precision | 1.000 | 0.232 | 0.926 | 0.230 | 0.156 |\n",
    "| recall | 0.146 | 0.001 | 0.511 | 0.001 | 0.072 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
