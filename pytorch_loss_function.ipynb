{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type of Errors and Loss Functions in PyTorch\n",
    "We will delve into multiple kind of loss functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Absolute Error Loss (L1 loss)   \n",
    "Its a simple average of the absolute difference between the target value and the value predicted by the model. It is calculated as: \n",
    "$$\\frac{1}{n}\\sum_{i=1}^{n}|y_{i}-\\hat{y}_{i}|$$   \n",
    "\n",
    "This could be used for regression problems. Due to MAE absolute nature is performing better to handle outliers than MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[ 0.4343,  0.9707,  1.5393,  0.2493,  1.3898],\n",
      "        [ 0.6490, -0.6834,  1.5204,  0.8786, -0.0912],\n",
      "        [ 1.3075,  0.3545,  0.1328, -0.3996, -0.0953],\n",
      "        [-0.0098, -0.0437, -1.0588,  0.6889,  0.6560],\n",
      "        [-1.3469, -1.1643, -0.0500, -0.8692,  0.7422]], requires_grad=True)\n",
      "target:  tensor([[ 1.8299, -1.4373, -0.0747,  0.1170,  0.7938],\n",
      "        [-3.0914,  0.5931,  2.5211,  0.4198,  0.2509],\n",
      "        [ 0.2228, -0.0819, -0.8674,  0.0950,  0.2390],\n",
      "        [-0.9124, -0.0527, -1.1152, -0.3570, -0.6817],\n",
      "        [-0.9945, -1.9955,  0.0796, -0.6463, -0.0321]])\n",
      "output:  tensor(0.8791, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input = torch.randn(5, 5, requires_grad=True)\n",
    "target = torch.randn(5, 5)\n",
    "\n",
    "# Mean Absolute Error\n",
    "mae = nn.L1Loss()\n",
    "output = mae(input, target) \n",
    "output.backward()\n",
    "\n",
    "print('input: ', input) \n",
    "print('target: ', target)\n",
    "print('output: ', output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error Loss (L2 Loss)   \n",
    "Its a simple average of the squared difference between the target value and the value predicted by the model. It is calculated as: \n",
    "$$\\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}$$\n",
    "\n",
    "This model punishes the model making a big mistakes (wrong prediction) and encourage a small mistakes. Good use for regression problem and default loss function for most Pytorch regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[ 0.4343,  0.9707,  1.5393,  0.2493,  1.3898],\n",
      "        [ 0.6490, -0.6834,  1.5204,  0.8786, -0.0912],\n",
      "        [ 1.3075,  0.3545,  0.1328, -0.3996, -0.0953],\n",
      "        [-0.0098, -0.0437, -1.0588,  0.6889,  0.6560],\n",
      "        [-1.3469, -1.1643, -0.0500, -0.8692,  0.7422]], requires_grad=True)\n",
      "target:  tensor([[ 1.8299, -1.4373, -0.0747,  0.1170,  0.7938],\n",
      "        [-3.0914,  0.5931,  2.5211,  0.4198,  0.2509],\n",
      "        [ 0.2228, -0.0819, -0.8674,  0.0950,  0.2390],\n",
      "        [-0.9124, -0.0527, -1.1152, -0.3570, -0.6817],\n",
      "        [-0.9945, -1.9955,  0.0796, -0.6463, -0.0321]])\n",
      "output:  tensor(1.4232, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "output = mse_loss(input, target)\n",
    "output.backward()\n",
    "\n",
    "print('input: ', input) \n",
    "print('target: ', target)\n",
    "print('output: ', output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Negative Log-Likelihood Loss Function (NLLLoss) \n",
    "The NLL is applied only on models with softmax function as activation layers. Softmax is used to predict the probability of each class and the NLLLoss is used to calculate the negative log likelihood loss. It is calculated as: \n",
    "$$\\frac{1}{n}\\sum_{i=1}^{n}-y_{i}\\log(\\hat{y}_{i})$$\n",
    "\n",
    "This loss function punishes the model making correct prediction with smaller probabilities, and enocurage for making prediction with higher probabilities. It is used for multiclass classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[ 0.4343,  0.9707,  1.5393,  0.2493,  1.3898],\n",
      "        [ 0.6490, -0.6834,  1.5204,  0.8786, -0.0912],\n",
      "        [ 1.3075,  0.3545,  0.1328, -0.3996, -0.0953],\n",
      "        [-0.0098, -0.0437, -1.0588,  0.6889,  0.6560],\n",
      "        [-1.3469, -1.1643, -0.0500, -0.8692,  0.7422]], requires_grad=True)\n",
      "target:  tensor([0, 1, 2, 3, 4])\n",
      "output:  tensor(1.7902, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "target = torch.tensor([0, 1, 2, 3, 4])\n",
    "\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "nll_loss = nn.NLLLoss()\n",
    "output = nll_loss(m(input), target)\n",
    "output.backward()\n",
    "\n",
    "print('input: ', input) \n",
    "print('target: ', target)\n",
    "print('output: ', output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Cross-Entropy Loss Function (CrossEntropyLoss)\n",
    "This computes the difference between two probability distributions.In the context of classification, it is used to quantify the difference between the predicted probability distribution and the true distribution. Its written as:   \n",
    "$$\\frac{1}{n}\\sum_{i=1}^{n}-y_{i}\\log(\\hat{y}_{i})-(1-y_{i})\\log(1-\\hat{y}_{i})$$\n",
    "\n",
    "Where y is the true label and y_hat is the predicted label.\n",
    "\n",
    "The logarithmic terms in the BCE loss ensure that when the model is wrong, especially with high confidence, the penalty is severe. If the model is only slightly incorrect in its prediction, the BCE loss won't penalize it as heavily as when the model is very confident and wrong.   \n",
    "\n",
    "Consider Mean Squared Error (MSE) for a binary classification task. While MSE will penalize wrong predictions, the penalty for being confidently wrong isn't as severe as with BCE. Thus, BCE is more suited for tasks where we want the model not just to predict correctly but also with high confidence.   \n",
    "\n",
    "BCE becomes especially crucial in applications where being confidently wrong can have severe consequences. For instance, in medical diagnoses, a model that's very confident about an incorrect diagnosis could lead to inappropriate treatment or Spam SMS. In such cases, BCE is a better choice than MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[ 0.4343,  0.9707,  1.5393,  0.2493,  1.3898],\n",
      "        [ 0.6490, -0.6834,  1.5204,  0.8786, -0.0912],\n",
      "        [ 1.3075,  0.3545,  0.1328, -0.3996, -0.0953],\n",
      "        [-0.0098, -0.0437, -1.0588,  0.6889,  0.6560],\n",
      "        [-1.3469, -1.1643, -0.0500, -0.8692,  0.7422]], requires_grad=True)\n",
      "target:  tensor([1, 4, 2, 0, 4])\n",
      "output:  tensor(1.7043, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "target = torch.empty(5, dtype=torch.long).random_(5)\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "output = cross_entropy_loss(input, target)\n",
    "output.backward()\n",
    "\n",
    "print('input: ', input) \n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge Embedding Loss Function (Hinge Loss)   \n",
    "This compute the loss where the predicted output is a score between -1 and 1. It is used for training classifiers. It is calculated as:\n",
    "$$\\frac{1}{n}\\sum_{i=1}^{n}\\max(0,1-y_{i}\\hat{y}_{i})$$\n",
    "\n",
    "This could be used for classification problems whether two inputs are disimilar or similar. Also learning nonlinear embeddings or semi supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  tensor([[ 0.4343,  0.9707,  1.5393,  0.2493,  1.3898],\n",
      "        [ 0.6490, -0.6834,  1.5204,  0.8786, -0.0912],\n",
      "        [ 1.3075,  0.3545,  0.1328, -0.3996, -0.0953],\n",
      "        [-0.0098, -0.0437, -1.0588,  0.6889,  0.6560],\n",
      "        [-1.3469, -1.1643, -0.0500, -0.8692,  0.7422]], requires_grad=True)\n",
      "target:  tensor([[-5.0075e-01,  7.5170e-01, -1.0888e+00,  5.2347e-01,  2.0991e+00],\n",
      "        [-1.3888e+00,  2.0310e-02,  1.7041e+00, -8.8872e-01, -2.8956e-01],\n",
      "        [-5.3095e-02, -1.8192e+00,  1.1073e-01,  1.0953e+00, -9.6332e-01],\n",
      "        [ 1.5442e-03,  8.9922e-01, -4.1892e-01, -2.0913e+00, -9.8721e-01],\n",
      "        [ 8.7475e-01, -9.7459e-02,  1.0512e-01,  1.1073e+00, -3.4975e-01]])\n",
      "output:  tensor(1.0703, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "target = torch.randn(5, 5)\n",
    "\n",
    "hinge_loss = nn.HingeEmbeddingLoss()\n",
    "output = hinge_loss(input, target)\n",
    "output.backward()\n",
    "\n",
    "print('input: ', input) \n",
    "print('target: ', target)\n",
    "print('output: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Margin Ranking Loss Function (Margin Loss)\n",
    "This compute the loss / criterion to predict the relative distance two inputs x1 and x2. It is used for learning to rank. It is calculated as:\n",
    "$$\\frac{1}{n}\\sum_{i=1}^{n}\\max(0,-y_{i}(x_{1i}-x_{2i})+margin)$$\n",
    "\n",
    "This could be used for ranking problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input one: tensor([ 1.0745, -0.1528,  1.2359,  0.8667,  1.0300], requires_grad=True)\n",
      "input two: tensor([ 1.0977, -0.1903, -0.5532,  0.2529,  0.0613], requires_grad=True)\n",
      "target: tensor([1., 1., 1., 1., 1.])\n",
      "output: tensor(0.0047, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_one = torch.randn(5, requires_grad=True)\n",
    "input_two = torch.randn(5, requires_grad=True)\n",
    "target = torch.rand(5).sign()\n",
    "\n",
    "ranking_loss = nn.MarginRankingLoss()\n",
    "output = ranking_loss(input_one, input_two, target)\n",
    "output.backward()\n",
    "\n",
    "print(\"input one:\", input_one)\n",
    "print(\"input two:\", input_two)\n",
    "print(\"target:\", target)\n",
    "print(\"output:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
